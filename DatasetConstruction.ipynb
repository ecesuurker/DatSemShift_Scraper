{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the shifts dataset to only include \"Semantic Evolution\" and \"Microevolution\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Shifts.csv')\n",
    "\n",
    "SC_data = data[(data['Type'] == ' Semantic evolution') | (data['Type'] == ' Microevolution')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Shifts Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging the shifts that are cognates into the same row. The shifts that share a root word and followed the same meaning shift path are placed in the same row. The script takes the csv that includes the shifts, and groups the shifts that share a shift ID, source language, source lexeme and meaning together. Then, the second languages of the grouped shifts and the second lexemes are written into the same row. For instance, if ShiftXYZ includes shifts from a word from an ancestor language into several daughter languages, and the shifted meaning is the same across these languages, these shifts are presented as a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keeping only the necessary columns in the dataframe\n",
    "SC_data = SC_data[['ID', 'Type', 'Language_1', 'Lexeme_1', 'Meaning_1', 'Direction', 'Language_2', 'Lexeme_2', 'Meaning_2']]\n",
    "\n",
    "#Grouping the rows that have the same value in ID, Language 1, Lexeme 1 and Meaning 1 columns, and merging the Language 2 and \n",
    "#Lexeme 2 columns into the same row, every value separated by commas.\n",
    "merged = SC_data.groupby(['ID', 'Language_1', 'Lexeme_1', 'Meaning_1']).agg({\n",
    "    'Language_2': lambda x: ','.join(x.unique()),\n",
    "    'Lexeme_2':   lambda x: ','.join(x.unique()),\n",
    "    'Meaning_2':  'first',   \n",
    "    'Type':       'first',   \n",
    "    'Direction':  'first'\n",
    "}).reset_index()\n",
    "\n",
    "#Reordering the columns\n",
    "merged_reordered = merged.iloc[:, [0, 1, 2, 3, 8, 4, 5, 6]]\n",
    "\n",
    "merged_reordered.to_csv('MergedShifts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly sampling 200 rows from a dataframe for annotation. 100 of these rows are saved with all the information and the other 100 rows are saved without any language information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To ensure that the results are consistent across trial\n",
    "random.seed(3)\n",
    "\n",
    "#Importing the dataframe\n",
    "data = pd.read_csv('MergedShifts.csv')\n",
    "\n",
    "#Since this column is the row number, it is unique to each row. So it is used for sampling.\n",
    "#Putting all row numbers into a list\n",
    "id_nums = data[['Unnamed: 0']].values.tolist()\n",
    "\n",
    "#Since after 'tolist' items are put as list into list, this loop puts them as single items into a list\n",
    "ids = []\n",
    "for i in id_nums:\n",
    "    ids.append(i[0])\n",
    "\n",
    "#Sampling 200 row numbers\n",
    "sample_id = random.sample(ids, 200)\n",
    "\n",
    "#Sampling 100 row numbers from the sampled row numbers for the rows with all information\n",
    "id_lang = random.sample(sample_id, 100)\n",
    "\n",
    "#Removing the 100 sampled row numbers from all of the row numbers\n",
    "id_nolang = list(set(sample_id) - set(id_lang))\n",
    "\n",
    "#Saving the first sample of rows\n",
    "lang = data[data['Unnamed: 0'].isin(id_lang)]\n",
    "\n",
    "#Saving the second sample of rows\n",
    "nolang = data[data['Unnamed: 0'].isin(id_nolang)]\n",
    "\n",
    "#Removing the language information from the second sample and the row number column\n",
    "nolang = nolang[['ID', 'Meaning_1', 'Direction', 'Meaning_2']]\n",
    "\n",
    "#Removing the row number column from the first sample\n",
    "lang = lang[['ID', 'Language_1', 'Lexeme_1', 'Meaning_1', 'Direction',\n",
    "       'Language_2', 'Lexeme_2', 'Meaning_2']]\n",
    "\n",
    "#Saving the sample with language information\n",
    "lang.to_csv('RandomShiftsLang.csv')\n",
    "\n",
    "#Saving the sample without language information\n",
    "nolang.to_csv('RandomShifts.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
